/*
 * This file and its contents are supplied under the terms of the
 * Common Development and Distribution License ("CDDL"), version 1.0.
 * You may only use this file in accordance with the terms of version
 * 1.0 of the CDDL.
 *
 * A full copy of the text of the CDDL should have accompanied this
 * source.  A copy of the CDDL is also available via the Internet at
 * http://www.illumos.org/license/CDDL.
 */

/*
 * Copyright 2022 Oxide Computer Company
 */

/*
 * This file implements a collection of routines that can be used to initialize
 * various aspects of the Milan CPU cores.
 */

#include <milan/milan_physaddrs.h>
#include <sys/io/milan/fabric.h>
#include <sys/io/milan/ccx.h>
#include <sys/io/milan/ccx_impl.h>
#include <sys/amdzen/ccx.h>
#include <sys/boot_physmem.h>
#include <sys/x86_archext.h>
#include <sys/types.h>

/*
 * We run before kmdb loads, so these chicken switches are static consts.
 */
static const boolean_t milan_ccx_allow_unsupported_processor = B_FALSE;

/*
 * Set the contents of undocumented registers to what we imagine they should be.
 * This chicken switch and the next exist mainly to debug total mysteries, but
 * it's also entirely possible that our sketchy information about what these
 * should hold is just wrong (for this machine, or entirely).
 */
static const boolean_t milan_ccx_set_undoc_regs = B_TRUE;

/*
 * Set the contents of undocumented fields in otherwise documented registers to
 * what we imagine they should be.
 */
static const boolean_t milan_ccx_set_undoc_fields = B_TRUE;

void
milan_ccx_mmio_init(uint64_t pa, boolean_t reserve)
{
	uint64_t val = AMD_MMIOCFG_BASEADDR_ENABLE;
	val |= AMD_MMIOCFG_BASEADDR_BUSRANGE_256 <<
	    AMD_MMIOCFG_BASEADDR_BUSRANGE_SHIFT;
	val |= (pa & AMD_MMIOCFG_BASEADDR_MASK);
	wrmsr(MSR_AMD_MMIOCFG_BASEADDR, val);

	if (reserve) {
		eb_physmem_reserve_range(pa,
		    (1UL << AMD_MMIOCFG_BASEADDR_BUSRANGE_256) <<
		    AMD_MMIOCFG_BASEADDR_ADDR_SHIFT, EBPR_NOT_RAM);
	}
}

void
milan_ccx_physmem_init(void)
{
	/*
	 * Due to undocumented, unspecified, and unknown bugs in the IOMMU
	 * (supposedly), there is a hole in RAM below 1 TiB.  It may or may not
	 * be usable as MMIO space but regardless we need to not treat it as
	 * RAM.
	 */
	eb_physmem_reserve_range(MILAN_PHYSADDR_IOMMU_HOLE,
	    MILAN_PHYSADDR_IOMMU_HOLE_END - MILAN_PHYSADDR_IOMMU_HOLE,
	    EBPR_NOT_RAM);
}

/*
 * In this context, "thread" == AP.  SMT may or may not be enabled (by HW, FW,
 * or our own controls).  That may affect the number of threads per core, but
 * doesn't otherwise change anything here.
 *
 * This function is one-way; once a thread has been enabled, we are told that
 * we must never clear this bit.  What happens if we do, I do not know.  If the
 * thread was already booted, this function does nothing and returns B_FALSE;
 * otherwise it returns B_TRUE and the AP will be started.  There is no way to
 * fail; we don't construct a milan_thread_t for hardware that doesn't exist, so
 * it's always possible to perform this operation if what we are handed points
 * to genuine data.
 *
 * See MP boot theory in os/mp_startup.c
 */
boolean_t
milan_ccx_start_thread(const milan_thread_t *thread)
{
	milan_core_t *core = thread->mt_core;
	milan_ccx_t *ccx = core->mc_ccx;
	milan_ccd_t *ccd = ccx->mcx_ccd;
	uint8_t thr_ccd_idx;
	uint32_t en;

	VERIFY3U(CPU->cpu_id, ==, 0);

	thr_ccd_idx = ccx->mcx_logical_cxno;
	thr_ccd_idx *= ccx->mcx_ncores;
	thr_ccd_idx += core->mc_logical_coreno;
	thr_ccd_idx *= core->mc_nthreads;
	thr_ccd_idx += thread->mt_threadno;

	VERIFY3U(thr_ccd_idx, <, MILAN_MAX_CCXS_PER_CCD *
	    MILAN_MAX_CORES_PER_CCX * MILAN_MAX_THREADS_PER_CORE);

	en = milan_ccd_smupwr_read32(ccd, MILAN_SMUPWR_R_SMN_THREAD_ENABLE);
	if (MILAN_SMUPWR_R_GET_THREAD_ENABLE_T(en, thr_ccd_idx) != 0)
		return (B_FALSE);

	en = MILAN_SMUPWR_R_SET_THREAD_ENABLE_T(en, thr_ccd_idx);
	milan_ccd_smupwr_write32(ccd, MILAN_SMUPWR_R_SMN_THREAD_ENABLE, en);
	return (B_TRUE);
}

apicid_t
milan_thread_apicid(const milan_thread_t *thread)
{
	return (thread->mt_apicid);
}

boolean_t
milan_ccx_is_supported(void)
{
	x86_chiprev_t chiprev;

	if (milan_ccx_allow_unsupported_processor)
		return (B_TRUE);

	chiprev = cpuid_getchiprev(CPU);
	return (chiprev_matches(chiprev, X86_CHIPREV_AMD_MILAN_ANY));
}

/*
 * This series of CCX subsystem initialisation routines is intended to
 * eventually be generalised out of Milan to support arbitrary future
 * collections of processors.  Each sets up a particular functional unit within
 * the thread/core/core complex.  For reference, these are:
 *
 * LS: load-store, the gateway to the thread
 * IC: (L1) instruction cache
 * DC: (L1) data cache
 * TW: table walker (part of the MMU)
 * DE: instruction decode(/execute?)
 * L2, L3: caches
 * UC: microcode -- this is not microcode patch/upgrade
 *
 * Feature initialisation refers to setting up the internal registers that are
 * reflected into cpuid leaf values.
 *
 * All of these routines are infallible; we purposely avoid using on_trap() or
 * similar as we want to panic if any of these registers does not exist or
 * cannot be accessed.  Additionally, when building with DEBUG enabled, we will
 * panic if writing the bits we intend to change is ineffective.  None of these
 * outcomes should ever be possible on a supported processor; indeed,
 * understanding what to do here is a critical element of adding support for a
 * new processor family or revision.
 */

static inline void
wrmsr_and_test(uint32_t msr, uint64_t v)
{
	wrmsr(msr, v);

#ifdef	DEBUG
	uint64_t rv = rdmsr(msr);

	if (rv != v) {
		cmn_err(CE_PANIC, "MSR 0x%x written with value 0x%lx "
		    "has value 0x%lx\n", msr, v, rv);
	}
#endif
}

static void
milan_thread_feature_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);
	x86_uarchrev_t uarchrev = cpuid_getuarchrev(CPU);

	v = rdmsr(MSR_AMD_CPUID_7_FEATURES);
	v = AMD_CPUID_7_FEATURES_SET_RTM(v, 0);
	v = AMD_CPUID_7_FEATURES_SET_HLE(v, 0);
	if (chiprev_matches(chiprev, X86_CHIPREV_AMD_MILAN_B0))
		v = AMD_CPUID_7_FEATURES_SET_ERMS(v, 0);
	else
		v = AMD_CPUID_7_FEATURES_SET_ERMS(v, 1);

	wrmsr_and_test(MSR_AMD_CPUID_7_FEATURES, v);

	v = rdmsr(MSR_AMD_FEATURE_EXT_ID);
	/*
	 * XXX Is IBS enable/disable an immutable boot-time policy?  If so, and
	 * if we want to allow controlling it, change this to reflect policy.
	 */
	if (milan_ccx_set_undoc_fields) {
		v = AMD_FEATURE_EXT_ID_SET_UNKNOWN_IBS_31(v, 0);
		v = AMD_FEATURE_EXT_ID_SET_UNKNOWN_22(v, 0);
	}

	wrmsr_and_test(MSR_AMD_FEATURE_EXT_ID, v);

	v = rdmsr(MSR_AMD_FEATURE_EXT2_EAX);
	v = AMD_FEATURE_EXT2_EAX_SET_NULL_SELECTOR_CLEARS_BASE(v, 1);
	if (milan_ccx_set_undoc_fields &&
	    (uarchrev_matches(uarchrev, X86_UARCHREV_AMD_ZEN3_B0) ||
	    chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0))) {
		v = AMD_FEATURE_EXT2_EAX_U_ZEN3_B0_SET_UNKNOWN_4(v, 0);
	}

	wrmsr_and_test(MSR_AMD_FEATURE_EXT2_EAX, v);

	if (uarchrev_at_least(uarchrev, X86_UARCHREV_AMD_ZEN3_B0)) {
		v = rdmsr(MSR_AMD_STRUCT_EXT_FEAT_ID_EDX0_ECX0);
		v = AMD_STRUCT_EXT_FEAT_ID_EDX0_ECX0_SET_FSRM(v, 1);

		wrmsr_and_test(MSR_AMD_STRUCT_EXT_FEAT_ID_EDX0_ECX0, v);
	}
}

static void
milan_thread_uc_init(void)
{
	uint64_t v;

	v = rdmsr(MSR_AMD_MCODE_CTL);
	v = AMD_MCODE_CTL_SET_REP_STOS_ST_THRESH(v,
	    AMD_MCODE_CTL_ST_THRESH_32M);
	v = AMD_MCODE_CTL_SET_REP_MOVS_ST_THRESH(v,
	    AMD_MCODE_CTL_ST_THRESH_32M);

	wrmsr_and_test(MSR_AMD_MCODE_CTL, v);
}

static void
milan_core_ls_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);

	v = rdmsr(MSR_AMD_LS_CFG);
	v = AMD_LS_CFG_SET_TEMP_LOCK_CONT_THRESH(v, 1);
	v = AMD_LS_CFG_SET_ALLOW_NULL_SEL_BASE_LIMIT_UPD(v, 1);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B1)) {
		v = AMD_LS_CFG_SET_SBEX_MISALIGNED_TLBMISS_MA1_FRC_MA2(v, 1);
	} else {
		v = AMD_LS_CFG_SET_SBEX_MISALIGNED_TLBMISS_MA1_FRC_MA2(v, 0);
	}
	/*
	 * XXX Possible boot-time or per-thread/guest policy option.
	 */
	v = AMD_LS_CFG_SET_DIS_STREAM_ST(v, 0);

	wrmsr_and_test(MSR_AMD_LS_CFG, v);

	v = rdmsr(MSR_AMD_LS_CFG2);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = AMD_LS_CFG2_SET_DIS_ST_PIPE_COMP_BYP(v, 0);
		v = AMD_LS_CFG2_SET_DIS_FAST_TPR_OPT(v, 0);
		v = AMD_LS_CFG2_SET_HW_PF_ST_PIPE_PRIO_SEL(v, 3);
	} else {
		v = AMD_LS_CFG2_SET_DIS_ST_PIPE_COMP_BYP(v, 1);
		v = AMD_LS_CFG2_SET_DIS_FAST_TPR_OPT(v, 1);
		v = AMD_LS_CFG2_SET_HW_PF_ST_PIPE_PRIO_SEL(v, 1);
	}

	wrmsr_and_test(MSR_AMD_LS_CFG2, v);

	v = rdmsr(MSR_AMD_LS_CFG3);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0) &&
	    milan_ccx_set_undoc_fields) {
		v = AMD_LS_CFG3_SET_UNKNOWN_62(v, 0);
		v = AMD_LS_CFG3_SET_UNKNOWN_56(v, 0);
		v = AMD_LS_CFG3_SET_DIS_NC_FILLWITH_LTLI(v, 0);
		/* XXX Possible policy option on B0+ only. */
		v = AMD_LS_CFG3_SET_EN_SPEC_ST_FILL(v, 1);
		v = AMD_LS_CFG3_SET_DIS_FAST_LD_BARRIER(v, 0);
	} else if (milan_ccx_set_undoc_fields) {
		v = AMD_LS_CFG3_SET_UNKNOWN_62(v, 1);
		v = AMD_LS_CFG3_SET_UNKNOWN_56(v, 1);
		v = AMD_LS_CFG3_SET_DIS_NC_FILLWITH_LTLI(v, 1);
		v = AMD_LS_CFG3_SET_EN_SPEC_ST_FILL(v, 0);
	}
	if (milan_ccx_set_undoc_fields) {
		v = AMD_LS_CFG3_SET_UNKNOWN_60(v, 1);
		v = AMD_LS_CFG3_SET_UNKNOWN_57(v, 1);
	}
	v = AMD_LS_CFG3_SET_DIS_SPEC_WC_NON_STRM_LD(v, 1);
	v = AMD_LS_CFG3_SET_DIS_MAB_FULL_SLEEP(v, 1);
	v = AMD_LS_CFG3_SET_DVM_SYNC_ONLY_ON_TLBI(v, 1);

	wrmsr_and_test(MSR_AMD_LS_CFG3, v);

	if (!chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = rdmsr(MSR_AMD_LS_CFG4);
		v = AMD_LS_CFG4_SET_DIS_LIVE_LOCK_CNT_FST_BUSLOCK(v, 1);
		v = AMD_LS_CFG4_SET_LIVE_LOCK_DET_FORCE_SBEX(v, 1);

		wrmsr_and_test(MSR_AMD_LS_CFG4, v);
	}
}

static void
milan_core_ic_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);

	v = rdmsr(MSR_AMD_IC_CFG);
	if (milan_ccx_set_undoc_fields) {
		if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
			v = AMD_IC_CFG_SET_UNKNOWN_48(v, 0);
		} else {
			v = AMD_IC_CFG_SET_UNKNOWN_48(v, 1);
			v = AMD_IC_CFG_SET_DIS_SPEC_TLB_RLD(v, 1);
			v = AMD_IC_CFG_SET_UNKNOWN_8(v, 0);
		}
		v = AMD_IC_CFG_SET_UNKNOWN_53(v, 0);
		v = AMD_IC_CFG_SET_UNKNOWN_52(v, 1);
		v = AMD_IC_CFG_SET_UNKNOWN_51(v, 1);
		v = AMD_IC_CFG_SET_UNKNOWN_50(v, 0);
	}
	/* XXX Possible policy option. */
	v = AMD_IC_CFG_SET_OPCACHE_DIS(v, 0);

	wrmsr_and_test(MSR_AMD_IC_CFG, v);
}

static void
milan_core_dc_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);

	/* XXX All of the prefetch controls may become policy options. */
	v = rdmsr(MSR_AMD_DC_CFG);
	v = AMD_DC_CFG_SET_DIS_REGION_HW_PF(v, 0);
	v = AMD_DC_CFG_SET_DIS_STRIDE_HW_PF(v, 0);
	v = AMD_DC_CFG_SET_DIS_STREAM_HW_PF(v, 0);
	v = AMD_DC_CFG_SET_DIS_PF_HW_FOR_SW_PF(v, 0);
	v = AMD_DC_CFG_SET_DIS_HW_PF(v, 0);

	wrmsr_and_test(MSR_AMD_DC_CFG, v);

	v = rdmsr(MSR_AMD_DC_CFG2);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = AMD_DC_CFG2_SET_DIS_DMB_STORE_LOCK(v, 0);
	} else {
		v = AMD_DC_CFG2_SET_DIS_DMB_STORE_LOCK(v, 1);
	}
	v = AMD_DC_CFG2_SET_DIS_SCB_NTA_L1(v, 1);

	wrmsr_and_test(MSR_AMD_DC_CFG2, v);
}

static void
milan_core_tw_init(void)
{
	uint64_t v;

	v = rdmsr(MSR_AMD_TW_CFG);
	v = AMD_TW_CFG_SET_COMBINE_CR0_CD(v, 1);

	wrmsr_and_test(MSR_AMD_TW_CFG, v);
}

static void
milan_core_de_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);

	v = rdmsr(MSR_AMD_DE_CFG);
	if (chiprev_matches(chiprev, X86_CHIPREV_AMD_MILAN_B0) &&
	    milan_ccx_set_undoc_fields) {
		v = AMD_DE_CFG_SET_UNKNOWN_60(v, 0);
		v = AMD_DE_CFG_SET_UNKNOWN_59(v, 0);
	} else if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B1) &&
	    milan_ccx_set_undoc_fields) {
		v = AMD_DE_CFG_SET_UNKNOWN_48(v, 1);
	} else if (milan_ccx_set_undoc_fields) {
		/* Older than B0 */
		v = AMD_DE_CFG_SET_UNKNOWN_60(v, 1);
		v = AMD_DE_CFG_SET_UNKNOWN_59(v, 1);
	}
	if (milan_ccx_set_undoc_fields) {
		v = AMD_DE_CFG_SET_UNKNOWN_33(v, 1);
		v = AMD_DE_CFG_SET_UNKNOWN_32(v, 1);
		v = AMD_DE_CFG_SET_UNKNOWN_28(v, 1);
	}

	wrmsr_and_test(MSR_AMD_DE_CFG, v);
}

static void
milan_core_l2_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);
	x86_uarch_t uarchrev = cpuid_getuarchrev(CPU);

	v = rdmsr(MSR_AMD_L2_CFG);
	v = AMD_L2_CFG_SET_DIS_HWA(v, 1);
	v = AMD_L2_CFG_SET_DIS_L2_PF_LOW_ARB_PRIORITY(v, 1);
	v = AMD_L2_CFG_SET_EXPLICIT_TAG_L3_PROBE_LOOKUP(v, 1);

	wrmsr_and_test(MSR_AMD_L2_CFG, v);

	/* XXX Prefetch policy options. */
	v = rdmsr(MSR_AMD_CH_L2_PF_CFG);
	v = AMD_CH_L2_PF_CFG_SET_EN_UP_DOWN_PF(v, 1);
	v = AMD_CH_L2_PF_CFG_SET_EN_STREAM_PF(v, 1);

	wrmsr_and_test(MSR_AMD_CH_L2_PF_CFG, v);

	v = rdmsr(MSR_AMD_CH_L2_CFG1);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0) &&
	    uarchrev_at_least(uarchrev, X86_UARCHREV_AMD_ZEN3_B0)) {
		v = AMD_CH_L2_CFG1_U_ZEN3_B0_SET_EN_BUSLOCK_IFETCH(v, 0);
	}
	v = AMD_CH_L2_CFG1_SET_EN_WCB_CONTEXT_DELAY(v, 1);
	v = AMD_CH_L2_CFG1_SET_CBB_MASTER_EN(v, 0);
	v = AMD_CH_L2_CFG1_SET_EN_PROBE_INTERRUPT(v, 1);
	v = AMD_CH_L2_CFG1_SET_EN_MIB_TOKEN_DELAY(v, 1);
	v = AMD_CH_L2_CFG1_SET_EN_MIB_THROTTLING(v, 1);

	wrmsr_and_test(MSR_AMD_CH_L2_CFG1, v);

	v = rdmsr(MSR_AMD_CH_L2_AA_CFG);
	v = AMD_CH_L2_AA_CFG_SET_SCALE_DEMAND(v, AMD_CH_L2_AA_CFG_SCALE_MUL4);
	v = AMD_CH_L2_AA_CFG_SET_SCALE_MISS_L3(v, AMD_CH_L2_AA_CFG_SCALE_MUL4);
	v = AMD_CH_L2_AA_CFG_SET_SCALE_MISS_L3_BW(v,
	    AMD_CH_L2_AA_CFG_SCALE_MUL4);
	v = AMD_CH_L2_AA_CFG_SET_SCALE_REMOTE(v, AMD_CH_L2_AA_CFG_SCALE_MUL4);

	wrmsr_and_test(MSR_AMD_CH_L2_AA_CFG, v);

	v = rdmsr(MSR_AMD_CH_L2_AA_PAIR_CFG0);
	v = AMD_CH_L2_AA_PAIR_CFG0_SET_SUPPRESS_DIFF_VICT(v, 1);

	wrmsr_and_test(MSR_AMD_CH_L2_AA_PAIR_CFG0, v);

	v = rdmsr(MSR_AMD_CH_L2_AA_PAIR_CFG1);
	v = AMD_CH_L2_AA_PAIR_CFG1_SET_DEMAND_HIT_PF_RRIP(v, 0);
	v = AMD_CH_L2_AA_PAIR_CFG1_SET_NOT_UNUSED_PF_RRIP_LVL_B4_L1V(v, 1);

	wrmsr_and_test(MSR_AMD_CH_L2_AA_PAIR_CFG1, v);
}

static void
milan_ccx_l3_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);
	x86_uarch_t uarchrev = cpuid_getuarchrev(CPU);

	v = rdmsr(MSR_AMD_CH_L3_CFG0);
	if (uarchrev_at_least(uarchrev, X86_UARCHREV_AMD_ZEN3_B1)) {
		v = AMD_CH_L3_CFG0_U_ZEN3_B1_SET_REPORT_SHARED_VIC(v, 1);
	}
	v = AMD_CH_L3_CFG0_SET_REPORT_RESPONSIBLE_VIC(v, 1);

	wrmsr_and_test(MSR_AMD_CH_L3_CFG0, v);

	v = rdmsr(MSR_AMD_CH_L3_CFG1);
	v = AMD_CH_L3_CFG1_SET_SDR_USE_L3_HIT_FOR_WASTED(v, 0);
	v = AMD_CH_L3_CFG1_SET_SDR_IF_DIS(v, 1);
	v = AMD_CH_L3_CFG1_SET_SDR_BURST_LIMIT(v,
	    AMD_CH_L3_CFG1_SDR_BURST_LIMIT_2_IN_16);
	v = AMD_CH_L3_CFG1_SET_SDR_DYN_SUP_NEAR(v, 0);
	v = AMD_CH_L3_CFG1_SET_SDR_LS_WASTE_THRESH(v,
	    AMD_CH_L3_CFG1_SDR_THRESH_255);
	v = AMD_CH_L3_CFG1_SET_SDR_IF_WASTE_THRESH(v,
	    AMD_CH_L3_CFG1_SDR_THRESH_255);

	wrmsr_and_test(MSR_AMD_CH_L3_CFG1, v);

	v = rdmsr(MSR_AMD_CH_L3_XI_CFG0);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = AMD_CH_L3_XI_CFG0_SET_SDR_REQ_BUSY_THRESH(v,
		    AMD_CH_L3_XI_CFG0_SDR_REQ_BUSY_THRESH_767);
	}
	v = AMD_CH_L3_XI_CFG0_SET_SDP_REQ_WR_SIZED_COMP_EN(v, 1);
	v = AMD_CH_L3_XI_CFG0_SET_SDP_REQ_VIC_BLK_COMP_EN(v, 1);
	v = AMD_CH_L3_XI_CFG0_SET_SDP_REQ_WR_SIZED_ZERO_EN(v, 1);
	v = AMD_CH_L3_XI_CFG0_SET_SDP_REQ_VIC_BLK_ZERO_EN(v, 1);
	v = AMD_CH_L3_XI_CFG0_SET_SDR_HIT_SPEC_FEEDBACK_EN(v, 1);
	v = AMD_CH_L3_XI_CFG0_SET_SDR_WASTE_THRESH(v,
	    AMD_CH_L3_XI_CFG0_SDR_THRESH_191);
	v = AMD_CH_L3_XI_CFG0_SET_SDR_SAMP_INTERVAL(v,
	    AMD_CH_L3_XI_CFG0_SDR_SAMP_INTERVAL_16K);

	wrmsr_and_test(MSR_AMD_CH_L3_XI_CFG0, v);
}

static void
milan_core_undoc_init(void)
{
	uint64_t v;
	x86_chiprev_t chiprev = cpuid_getchiprev(CPU);

	if (!milan_ccx_set_undoc_regs)
		return;

	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = rdmsr(MSR_AMD_UNKNOWN_C001_102C);
		v = AMD_UNKNOWN_C001_102C_SET_UNKNOWN_58(v, 1);

		wrmsr_and_test(MSR_AMD_UNKNOWN_C001_102C, v);
	}

	v = rdmsr(MSR_AMD_BP_CFG);
	if (chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B0)) {
		v = AMD_BP_CFG_SET_UNKNOWN_14(v, 0);
		v = AMD_BP_CFG_SET_UNKNOWN_6(v, 1);
		v = AMD_BP_CFG_SET_UNKNOWN_1(v, 0);
	} else {
		v = AMD_BP_CFG_SET_UNKNOWN_14(v, 1);
		v = AMD_BP_CFG_SET_UNKNOWN_6(v, 0);
		v = AMD_BP_CFG_SET_UNKNOWN_1(v, 1);
	}
	/* Override B0 setting for UNKNOWN_5 */
	if (chiprev_matches(chiprev, X86_CHIPREV_AMD_MILAN_A0) ||
	    chiprev_at_least(chiprev, X86_CHIPREV_AMD_MILAN_B1)) {
		v = AMD_BP_CFG_SET_UNKNOWN_5(v, 1);
	}
	v = AMD_BP_CFG_SET_UNKNOWN_4_2(v, 0);

	wrmsr_and_test(MSR_AMD_BP_CFG, v);
}

void
milan_ccx_init(void)
{
	const milan_thread_t *thread = CPU->cpu_m.mcpu_hwthread;
	char str[CPUID_BRANDSTR_STRLEN + 1];

	/*
	 * First things first: it shouldn't be (and generally isn't) possible to
	 * get here on a completely bogus CPU; e.g., Intel or a pre-Zen part.
	 * But the remainder of this function, and our overall body of code,
	 * support only a limited subset of processors that exist.  Eventually
	 * this will include processors that are not Milan, and at that time
	 * this set of checks will need to be factored out; even so, we also
	 * want to make sure we're on a supported revision.  A chicken switch is
	 * available to ease future porting work.
	 */

	if (!milan_ccx_is_supported()) {
		uint_t vendor, family, model, step;

		vendor = cpuid_getvendor(CPU);
		family = cpuid_getfamily(CPU);
		model = cpuid_getmodel(CPU);
		step = cpuid_getstep(CPU);
		panic("cpu%d is unsupported: vendor 0x%x family 0x%x "
		    "model 0x%x step 0x%x\n", CPU->cpu_id,
		    vendor, family, model, step);
	}

	/*
	 * Set the MSRs that control the brand string so that subsequent cpuid
	 * passes can retrieve it.  We fetched it from the SMU during earlyboot
	 * fabric initialisation.
	 */
	if (milan_fabric_thread_get_brandstr(thread, str, sizeof (str)) <=
	    CPUID_BRANDSTR_STRLEN && str[0] != '\0') {
		for (uint_t n = 0; n < sizeof (str) / sizeof (uint64_t); n++) {
			uint64_t sv = *(uint64_t *)&str[n * sizeof (uint64_t)];

			wrmsr(MSR_AMD_PROC_NAME_STRING0 + n, sv);
		}
	} else {
		cmn_err(CE_WARN, "cpu%d: SMU provided invalid brand string\n",
		    CPU->cpu_id);
	}

	/*
	 * We're called here from every thread, but the CCX doesn't have an
	 * instance of every functional unit for each thread.  As an
	 * optimisation, we set up what's shared only once.  One would imagine
	 * that the sensible way to go about that is to always perform the
	 * initialisation on the first thread that shares the functional unit,
	 * but other implementations do it only on the last.  It's possible that
	 * this is a bug, or that the internal process of starting a thread
	 * clobbers (some of?) the changes we might make to the shared register
	 * instances before doing so.  On the processors we support, doing this
	 * on the first sharing thread to start seems to have the intended
	 * result, so that's what we do.  Functions are named for their scope.
	 * The exception to the rule is the table walker configuration, which
	 * causes CR0.CD to be effectively set on both threads if either thread
	 * has it set; since by default, a thread1 that hasn't started yet has
	 * this bit set, setting it on thread0 will cause everything to grind to
	 * a near halt.  Since the TW config bit has no effect without SMT, we
	 * don't need to worry about setting it on thread0 if SMT is off.
	 */
	milan_thread_feature_init();
	milan_thread_uc_init();
	if (thread->mt_threadno == 1) {
		milan_core_tw_init();
	}
	if (thread->mt_threadno == 0) {
		milan_core_ls_init();
		milan_core_ic_init();
		milan_core_dc_init();
		milan_core_de_init();
		milan_core_l2_init();
		if (thread->mt_core->mc_logical_coreno == 0)
			milan_ccx_l3_init();
		milan_core_undoc_init();
	}
}
